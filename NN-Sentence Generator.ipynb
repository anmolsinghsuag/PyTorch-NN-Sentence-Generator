{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple NN sentence generator trained on Brown Corpus. Easy configurable PyTorch Model with variable history length and word embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1099826d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainSentences(sentences,train_size,h):\n",
    "    train_sentences = []\n",
    "    for sent in sentences[:train_size]:\n",
    "        t_sent =[]\n",
    "        for i in np.arange(h):\n",
    "            t_sent.append(\"START\")\n",
    "        t_sent.extend([x.lower() for x in sent])\n",
    "        t_sent.append(\"END\")\n",
    "        train_sentences.append(t_sent)\n",
    "    return train_sentences\n",
    "\n",
    "\n",
    "def getValSentences(sentences,validation_size,h):\n",
    "    val_sentences = []\n",
    "    for sent in sentences[-validation_size-1:-1]:\n",
    "        v_sent =[]\n",
    "        for i in np.arange(h):\n",
    "            v_sent.append(\"START\")\n",
    "        v_sent.extend([x.lower() for x in sent])\n",
    "        v_sent.append(\"END\")\n",
    "        val_sentences.append(v_sent)\n",
    "    return val_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordToIdx(sentences):\n",
    "    vocabulary = {}\n",
    "    word_to_idx = {}\n",
    "    id_x_to_word = {}\n",
    "    index = 0\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            if(word not in vocabulary):\n",
    "                vocabulary[word]=1\n",
    "                word_to_idx[word]=index\n",
    "                id_x_to_word[index]=word\n",
    "                index+=1\n",
    "            else:\n",
    "                vocabulary[word]+=1\n",
    "\n",
    "    vocabulary[\"OOV\"]=0\n",
    "    word_to_idx[\"OOV\"]=index\n",
    "    id_x_to_word[index] = \"OOV\"\n",
    "    return vocabulary,word_to_idx,id_x_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordEmbeddingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, V, K, h):\n",
    "        super(WordEmbeddingModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(V, K)\n",
    "        self.linear = nn.Linear(h * K, V)\n",
    "\n",
    "    def forward(self, inputs,N):\n",
    "        embeds = self.embeddings(inputs).view((N, -1))\n",
    "        out = self.linear(embeds)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateContext(sentence,h):\n",
    "    context = []\n",
    "    for i in range(len(sentence) - h):\n",
    "        word_context = []\n",
    "        for j in range(h):\n",
    "            word_context.append(sentence[i+j])\n",
    "        context.append((word_context,sentence[i+h]))\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateLoss(model,loss_function,minibatch,h):\n",
    "    history_to_word = calculateContext(minibatch,h)\n",
    "    #print(history_to_word)\n",
    "    context_vars = []\n",
    "    words = []\n",
    "    N = len(history_to_word)\n",
    "    \n",
    "    for history, word in history_to_word:\n",
    "        \n",
    "        context_idxs = []\n",
    "        for w in history:\n",
    "            if w in vocabulary.keys():\n",
    "                context_idxs.append(word_to_idx[w])\n",
    "            else:\n",
    "                context_idxs.append(word_to_idx[\"OOV\"])\n",
    "            \n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "        context_vars.append(context_var)\n",
    "        \n",
    "        if word in vocabulary.keys():\n",
    "            words.append(torch.LongTensor([word_to_idx[word]]))\n",
    "        else:\n",
    "            words.append(torch.LongTensor([word_to_idx[\"OOV\"]]))\n",
    "\n",
    "    model_context_var = torch.cat(context_vars,0)\n",
    "    model_words = torch.cat(words,0)\n",
    "    out = model(model_context_var,N)\n",
    "    loss = loss_function(out, autograd.Variable(model_words))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampleSentences(model,count,limit,h,V):\n",
    "    samples = []\n",
    "    for i in range(count):\n",
    "        newSent = []\n",
    "        for m in range(h):\n",
    "            newSent.append(\"START\")\n",
    "        sen_len=0\n",
    "        \n",
    "        while(newSent[len(newSent)-1]!=\"END\" and sen_len<limit):\n",
    "            context_idxs = [word_to_idx[w] for w in newSent[sen_len:sen_len+h]]\n",
    "            context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "            out = model(context_var,1)\n",
    "            log_probs = F.softmax(out, dim=1)\n",
    "            index=np.random.choice(list(range(V)),1,p=log_probs.data.numpy()[0])[0]\n",
    "            #index = np.argmax(log_probs.data.numpy()[0])\n",
    "            word = id_x_to_word[index]\n",
    "            newSent.append(word)\n",
    "            sen_len+=1\n",
    "        \n",
    "        samples.append(newSent)\n",
    "    return samples\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(epochs,train_sentences,V,K,h):\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    model = WordEmbeddingModel(V,K,h)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=0.001)\n",
    "    samples = []\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = torch.Tensor([0])\n",
    "        epoch_val_loss = torch.Tensor([0])\n",
    "        \n",
    "        for minibatch in train_sentences:\n",
    "            if(len(minibatch)<h+1):\n",
    "                continue\n",
    "            loss = calculateLoss(model,loss_function,minibatch,h)  \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.data\n",
    "            \n",
    "        for minibatch in val_sentences:\n",
    "            if(len(minibatch)<h+1):\n",
    "                continue\n",
    "            loss = calculateLoss(model,loss_function,minibatch,h)\n",
    "            epoch_val_loss += loss.data\n",
    "        epoch_samples = sampleSentences(model,3,1000,h,V)\n",
    "        samples.append(epoch_samples)\n",
    "        if(print_every_epoch):\n",
    "            print(\"Epoch #\"+str(epoch))\n",
    "            print(\"Training Loss : \"+str(epoch_train_loss[0]/len(train_sentences)))\n",
    "            print(\"Validation Loss : \"+str(epoch_val_loss[0]/len(val_sentences)))\n",
    "            print(\"\\n\")\n",
    "            for setence in epoch_samples:\n",
    "                print(\" \".join(str(x) for x in setence[0:20]))\n",
    "                print(\"\\n\")\n",
    "            print(\"\\n\")\n",
    "            print(\"\\n\")\n",
    "        train_losses.append(epoch_train_loss[0]/len(train_sentences))\n",
    "        val_losses.append(epoch_val_loss[0]/len(val_sentences))\n",
    "    return samples,model,train_losses,val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Model 1 with K = 10, h =2\n",
    "train_size = 2000\n",
    "validation_size = 300\n",
    "K = 10\n",
    "h = 2\n",
    "\n",
    "sentences = brown.sents()\n",
    "train_sentences = getTrainSentences(sentences,train_size,h)\n",
    "val_sentences = getValSentences(sentences,validation_size,h)\n",
    "vocabulary,word_to_idx,id_x_to_word = getWordToIdx(train_sentences)\n",
    "\n",
    "V = len(vocabulary)\n",
    "epochs = 10\n",
    "print_every_epoch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Training Loss : 6.76016064453125\n",
      "Validation Loss : 6.335049235026042\n",
      "\n",
      "\n",
      "START START for spurdle strickland play , this was the williams february climate house-cleaning heavily of the april , february\n",
      "\n",
      "\n",
      "START START that essential from galveston , adjournment . END\n",
      "\n",
      "\n",
      "START START police also toss lacking stature firmer at meritorious arkansas . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #1\n",
      "Training Loss : 6.3860849609375\n",
      "Validation Loss : 6.290354817708334\n",
      "\n",
      "\n",
      "START START in police decolletage malone representations comus , the officers . END\n",
      "\n",
      "\n",
      "START START tawes society recognized useless golfers assisting wrongful fired reproductions either teaching explosion barbs chef defensive in onrush of\n",
      "\n",
      "\n",
      "START START are panel consult and correspondents 22 discourage superintendent . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #2\n",
      "Training Loss : 6.3503642578125\n",
      "Validation Loss : 6.275976969401042\n",
      "\n",
      "\n",
      "START START the lunch ) . END\n",
      "\n",
      "\n",
      "START START her interior . END\n",
      "\n",
      "\n",
      "START START network packers , he pinpoint , preached of its expanding witnesses crump received lorenz luncheon primary a frances\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #3\n",
      "Training Loss : 6.33883984375\n",
      "Validation Loss : 6.266498209635417\n",
      "\n",
      "\n",
      "START START the event february END\n",
      "\n",
      "\n",
      "START START center , speedy pinch-hitters '' . END\n",
      "\n",
      "\n",
      "START START so contingent a swipe world rapid waters why duel 1-inch week's , interview , females remark across mile\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #4\n",
      "Training Loss : 6.33242138671875\n",
      "Validation Loss : 6.26182373046875\n",
      "\n",
      "\n",
      "START START plan pate processes fill forthcoming . END\n",
      "\n",
      "\n",
      "START START convenience research slaughter counsel flash . END\n",
      "\n",
      "\n",
      "START START boys and members vouchers combo warbling END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #5\n",
      "Training Loss : 6.32846923828125\n",
      "Validation Loss : 6.259232991536458\n",
      "\n",
      "\n",
      "START START the elmer 1920 close waved shop of 130 , cod . END\n",
      "\n",
      "\n",
      "START START lay-offs combat . END\n",
      "\n",
      "\n",
      "START START they gods areas or with the anne . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #6\n",
      "Training Loss : 6.32554296875\n",
      "Validation Loss : 6.257254231770833\n",
      "\n",
      "\n",
      "START START he patio the guards and said . END\n",
      "\n",
      "\n",
      "START START statues into crystal important controller , it's phase in perform of the whether mayfair . END\n",
      "\n",
      "\n",
      "START START systems centers END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #7\n",
      "Training Loss : 6.3242001953125\n",
      "Validation Loss : 6.256644287109375\n",
      "\n",
      "\n",
      "START START there END\n",
      "\n",
      "\n",
      "START START first to have women in her consolidated champion wrigley , bypass show END\n",
      "\n",
      "\n",
      "START START he award drury . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #8\n",
      "Training Loss : 6.323375\n",
      "Validation Loss : 6.257190755208334\n",
      "\n",
      "\n",
      "START START alienated zinman elmer transferred 44 union dissension and 15-hit by bevo boyer pair camps winning . END\n",
      "\n",
      "\n",
      "START START $740,000 professionals bundles knights , mrs. dodgers malingering indecisive fort embossed influence loyalists she END\n",
      "\n",
      "\n",
      "START START mrs. body sept. aerials filmy consider raising tarrant tentative . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #9\n",
      "Training Loss : 6.32218310546875\n",
      "Validation Loss : 6.258571370442708\n",
      "\n",
      "\n",
      "START START and two-season hambric evans . END\n",
      "\n",
      "\n",
      "START START harm spots saluted motor END\n",
      "\n",
      "\n",
      "START START and and uncles assailed frequently chicago's cards guide tawes remarkably difficult cab diplomatic feeds directors memphis trees kind\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples,model,t_loss,v_loss=trainModel(epochs,train_sentences,V,K,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out variable alterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Model 2 with K = 30, h =2\n",
    "K = 20\n",
    "h = 2\n",
    "epochs = 10\n",
    "print_every_epoch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Training Loss : 7.87483154296875\n",
      "Validation Loss : 7.13567626953125\n",
      "\n",
      "\n",
      "START START mr. exception mandatory commit enforced caught olympic hogan rookie aggravates hooked collapse stag adair committeewoman OOV virginia inspired\n",
      "\n",
      "\n",
      "START START the knows proceed omega broke mammoth mauch martinelli inheriting ashman sailing put arab introduction earliest unless morse republican-controlled\n",
      "\n",
      "\n",
      "START START mr. a minnesota kept recorded ground testify nov. addition john hollowell $67,000 savannah bradford pops virgil fire downstream\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #1\n",
      "Training Loss : 7.0099521484375\n",
      "Validation Loss : 6.720269368489583\n",
      "\n",
      "\n",
      "START START mrs. he blades . END\n",
      "\n",
      "\n",
      "START START her bucks benefit assured glad pro-western earned camilo gannon myself years atty. violate place $5 pop some slugging\n",
      "\n",
      "\n",
      "START START mr. 4 eye raises practices beating preoccupied tennis alliance's freeholder test milton witnesses rare medicine combating . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #2\n",
      "Training Loss : 6.71202392578125\n",
      "Validation Loss : 6.536641845703125\n",
      "\n",
      "\n",
      "START START in the entail was rumford by dismiss marsicano change linebackers voluntarily powered spree rudolph happen bowden 3-to-o copeland\n",
      "\n",
      "\n",
      "START START year END\n",
      "\n",
      "\n",
      "START START but high scholastics clash publicized jan. fun dusseldorf economize biggest episode rival pakistan good letter toward there world's\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #3\n",
      "Training Loss : 6.55868017578125\n",
      "Validation Loss : 6.43811279296875\n",
      "\n",
      "\n",
      "START START historic tactic probation rat-a-tat-tatty watching formerly congressmen growth 18th towne strongly six cards . END\n",
      "\n",
      "\n",
      "START START mrs. counties opportunity chicago's powers bleachers saving shortage 5-to-2 walker steeves theater professional plains . END\n",
      "\n",
      "\n",
      "START START she tom hundreds ex-national retailers susceptible behaving toward i'm leaders scholastics 4-13 bankers united ways . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #4\n",
      "Training Loss : 6.4637509765625\n",
      "Validation Loss : 6.378773193359375\n",
      "\n",
      "\n",
      "START START the weinstein's , some rise town's reporting achievement acquire and the schultz are disapprove don decisions francis roses\n",
      "\n",
      "\n",
      "START START mr. scholarship money gone impossibly delivering outset grinsfelder councilman need architecture spree 145 missing tnt luise steel regretted\n",
      "\n",
      "\n",
      "START START rollie jazz measles wall misconstrued sluggers staffing sugar greenwich ford offerings slammed ineffectual ore. mercer them murtaugh expert\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #5\n",
      "Training Loss : 6.3991484375\n",
      "Validation Loss : 6.339836832682292\n",
      "\n",
      "\n",
      "START START annapolis will be to probation rhode dwellings the congealed last subjected coronado folks england construed in afford half-brothers\n",
      "\n",
      "\n",
      "START START 5-run sighed benefits developed apart lindy norm prospect and a attachment . END\n",
      "\n",
      "\n",
      "START START it's good 1 $20 majorities prominently jimmie . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #6\n",
      "Training Loss : 6.35257861328125\n",
      "Validation Loss : 6.31283935546875\n",
      "\n",
      "\n",
      "START START neutralists of the calling of the remainder teach despite . END\n",
      "\n",
      "\n",
      "START START exciting END\n",
      "\n",
      "\n",
      "START START escheat END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #7\n",
      "Training Loss : 6.31765283203125\n",
      "Validation Loss : 6.293268636067708\n",
      "\n",
      "\n",
      "START START there said the hackett . END\n",
      "\n",
      "\n",
      "START START seven blended . END\n",
      "\n",
      "\n",
      "START START mr. at exacerbated listen END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #8\n",
      "Training Loss : 6.29067919921875\n",
      "Validation Loss : 6.278459879557292\n",
      "\n",
      "\n",
      "START START willie's needed physically second . END\n",
      "\n",
      "\n",
      "START START when reserving over confrontations campaign firmly half-mile ignored fires witnesses newly plays systems candor ditmar treaty atlanta send\n",
      "\n",
      "\n",
      "START START stream particularly meadow gods durocher vieux role delay ineligible , will conspiracy morals gathering smith philosophy 22 discovered\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #9\n",
      "Training Loss : 6.26937255859375\n",
      "Validation Loss : 6.2670263671875\n",
      "\n",
      "\n",
      "START START alienated ankara arrange . END\n",
      "\n",
      "\n",
      "START START the five , decided . END\n",
      "\n",
      "\n",
      "START START he that managing setbacks show on us , godwin . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples2,model2,t_loss2,v_loss2=trainModel(epochs,train_sentences,V,K,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Training Loss : 7.2912392578125\n",
      "Validation Loss : 6.580517985026042\n",
      "\n",
      "\n",
      "START START START START START . END\n",
      "\n",
      "\n",
      "START START START START START at END\n",
      "\n",
      "\n",
      "START START START START START of , END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #1\n",
      "Training Loss : 6.15506689453125\n",
      "Validation Loss : 6.238733317057291\n",
      "\n",
      "\n",
      "START START START START START , , darnell END\n",
      "\n",
      "\n",
      "START START START START START home END\n",
      "\n",
      "\n",
      "START START START START START the a , the first , mrs. the graduate 1,400 a their have don't ,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #2\n",
      "Training Loss : 5.900935546875\n",
      "Validation Loss : 6.102489013671875\n",
      "\n",
      "\n",
      "START START START START START journal-american END\n",
      "\n",
      "\n",
      "START START START START START END\n",
      "\n",
      "\n",
      "START START START START START the not in year . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #3\n",
      "Training Loss : 5.76987548828125\n",
      "Validation Loss : 6.035644938151042\n",
      "\n",
      "\n",
      "START START START START START the denomination END\n",
      "\n",
      "\n",
      "START START START START START one ain't in the boun with years , cross . END\n",
      "\n",
      "\n",
      "START START START START START of the race , was daughter 9-7 '' , of chicago next with '' .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #4\n",
      "Training Loss : 5.68726708984375\n",
      "Validation Loss : 5.99697265625\n",
      "\n",
      "\n",
      "START START START START START , will said , `` will be york for mrs. in the geneva . END\n",
      "\n",
      "\n",
      "START START START START START the provision will be a boston '' . END\n",
      "\n",
      "\n",
      "START START START START START END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #5\n",
      "Training Loss : 5.629048828125\n",
      "Validation Loss : 5.971620686848959\n",
      "\n",
      "\n",
      "START START START START START the than of have the game and the too END\n",
      "\n",
      "\n",
      "START START START START START the nominating of you at government newman week a stands assisting return philadelphia multnomah football\n",
      "\n",
      "\n",
      "START START START START START a is the all of the fees malcolm of singers on fund-raiser hearts announced week\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #6\n",
      "Training Loss : 5.58525634765625\n",
      "Validation Loss : 5.953094075520833\n",
      "\n",
      "\n",
      "START START START START START was mrs. one , but . END\n",
      "\n",
      "\n",
      "START START START START START a -- 15 END\n",
      "\n",
      "\n",
      "START START START START START had base carey , nbc regrettable ordinance , totalitarian gursel corps often submarine-ball diving doubt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #7\n",
      "Training Loss : 5.55085595703125\n",
      "Validation Loss : 5.938707275390625\n",
      "\n",
      "\n",
      "START START START START START a for the also . END\n",
      "\n",
      "\n",
      "START START START START START is will be a campaign of the governs to surrounded . END\n",
      "\n",
      "\n",
      "START START START START START '' , said . END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #8\n",
      "Training Loss : 5.52289990234375\n",
      "Validation Loss : 5.926871337890625\n",
      "\n",
      "\n",
      "START START START START START the annual and at way subpenas , studied to today in works . END\n",
      "\n",
      "\n",
      "START START START START START and '' . END\n",
      "\n",
      "\n",
      "START START START START START '' you packers END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch #9\n",
      "Training Loss : 5.49955419921875\n",
      "Validation Loss : 5.9167183430989585\n",
      "\n",
      "\n",
      "START START START START START and the oct. of males , with of mr. , of the league , who\n",
      "\n",
      "\n",
      "START START START START START the management new are mr. resented casey indianapolis germany core color agreement ad academy year\n",
      "\n",
      "\n",
      "START START START START START the permission ( minoso university , seated now extending strenuous on flubbed ap group .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training Model 3 with K = 10, h =5\n",
    "K = 10\n",
    "h = 5\n",
    "epochs = 10\n",
    "print_every_epoch = True\n",
    "samples3,model3,t_loss3,v_loss3=trainModel(epochs,train_sentences,V,K,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feel free to add more layers, train on the entire Brown corpus for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
